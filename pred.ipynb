{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pred.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOz0BTewPO2tzL39Uv55JyX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/returaj/cs6910/blob/assginment1_akash/pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp drive/MyDrive/DLASSIGNMENT/optimizer.py .\n",
        "import optimizer\n",
        "!cp drive/MyDrive/DLASSIGNMENT/activation_func.py .\n",
        "import activation_func"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLlf4wESp3Xc",
        "outputId": "d0a6d256-4ffd-4e8c-9d91-e2e013853acd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "class BaseLossFunc(object):\n",
        "  @staticmethod\n",
        "  def error(X, y, model):\n",
        "    raise NotImplementedError(\"error() method not implemented\")\n",
        "\n",
        "  @staticmethod\n",
        "  def grad(layer, y):\n",
        "    raise NotImplementedError(\"grad() method not implement\")\n",
        "class CrossEntropy(BaseLossFunc):\n",
        "  @staticmethod\n",
        "  def error(X, y, model):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    batch_size = X.shape[0]\n",
        "    prob = model.forward(X)[-1]\n",
        "    err = - np.sum(np.log(prob[np.arange(batch_size), y])) / batch_size\n",
        "    for w in model.weight:\n",
        "      err += model.reg * np.sum(w**2)\n",
        "    return err\n",
        "  @staticmethod\n",
        "  def predict(X,Y_true,model):\n",
        "    data_size=X.shape[0]\n",
        "    prob=model.forward(X)[-1]\n",
        "    Y_pred=[]\n",
        "    iter=0\n",
        "    for i in range(data_size):\n",
        "      Y_pred.append(np.argmax(prob[i]))\n",
        "    for j in range(data_size):\n",
        "      if Y_pred[j]==Y_true[j]:\n",
        "        iter=iter+1\n",
        "    accuracy=iter/data_size\n",
        "    accuracy=accuracy*100\n",
        "    print(f'accuracy is:{accuracy}')\n",
        "    return  Y_pred\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "  @staticmethod\n",
        "  def grad(layer, y):\n",
        "    \"\"\"\n",
        "    layer: (batch_size(B), output_size(O))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    batch_size = layer.shape[0]\n",
        "    dl = layer / batch_size\n",
        "    dl[np.arange(batch_size), y] -= 1/batch_size\n",
        "    return dl"
      ],
      "metadata": {
        "id": "sSjXXw-k3Ytx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from optimizer import SGD\n",
        "from activation_func import Sigmoid, Relu, Tanh\n",
        "class FNN(object):\n",
        "  def __init__(self, input_size, output_size, hidden_layers_size, act_func, loss_func, reg=0, init='random'):\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.weight, self.bias = None, None\n",
        "    self.act_func = act_func\n",
        "    self.loss_func = loss_func\n",
        "    self.reg = reg\n",
        "    self.initialize(input_size, hidden_layers_size, output_size, init.lower())\n",
        "\n",
        "  def initialize(self, input_size, hidden_layers_size, output_size, type):\n",
        "    self.weight, self.bias = [], []\n",
        "    prev_layer_size = input_size\n",
        "    hidden_layers_size.append(output_size)\n",
        "    for curr_layer_size in hidden_layers_size:\n",
        "      std = np.sqrt(prev_layer_size * curr_layer_size) if type == 'xavier' else 1\n",
        "      self.weight.append(np.random.rand(prev_layer_size, curr_layer_size)/std)\n",
        "      self.bias.append(np.zeros(curr_layer_size))\n",
        "      prev_layer_size = curr_layer_size\n",
        "\n",
        "  @staticmethod\n",
        "  def softmax(x):\n",
        "    \"\"\"\n",
        "    x: (batch_size(B), data_size(N))\n",
        "    \"\"\"\n",
        "    max_x = np.max(x, axis=1, keepdims=True)\n",
        "    exp_prob = np.exp(x - max_x)\n",
        "    prob = exp_prob / np.sum(exp_prob, axis=1, keepdims=True)\n",
        "    return prob\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    \"\"\"\n",
        "    layer_output = []\n",
        "    prev_layer = X\n",
        "    num_hidden_layers = last_layer = len(self.weight) - 1\n",
        "    for t in range(num_hidden_layers):\n",
        "      w, b = self.weight[t], self.bias[t]\n",
        "      next_layer = self.act_func.apply(np.dot(prev_layer, w) + b)\n",
        "      layer_output.append(next_layer)\n",
        "      prev_layer = next_layer\n",
        "    w, b = self.weight[last_layer], self.bias[last_layer]\n",
        "    prob = self.softmax(np.dot(prev_layer, w) + b)\n",
        "    layer_output.append(prob)\n",
        "    return layer_output\n",
        "\n",
        "  def backward(self, X, y, layer_output):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    batch_size, _ = X.shape\n",
        "    num_hidden_layers = last_layer = len(layer_output)-1\n",
        "    dw, db = [None]*(num_hidden_layers+1), [None]*(num_hidden_layers+1)\n",
        "    for t in range(num_hidden_layers, -1, -1):\n",
        "      if t == last_layer:\n",
        "        dh = self.loss_func.grad(layer_output[t], y)\n",
        "      else:\n",
        "        dh = np.dot(dh_fwd, self.weight[t+1].T) * self.act_func.grad(layer_output[t])\n",
        "      prev_layer_output = X if t==0 else layer_output[t-1]\n",
        "      dw[t] = np.dot(prev_layer_output.T, dh) + self.reg * self.weight[t]\n",
        "      db[t] = np.sum(dh, axis=0)\n",
        "      dh_fwd = dh\n",
        "    return dw, db\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  consider =100\n",
        "  X = np.array([(x_train[i].flatten())/255 for i in range(consider)]) / 255\n",
        "  Y = y_train[:consider]\n",
        "  batch_size, epochs = 16, 10\n",
        "  act_func, loss_func = Sigmoid(), CrossEntropy()\n",
        "  model = FNN(784, 10, [50, 20], act_func=act_func, loss_func=loss_func, init='random')\n",
        "  sgd = SGD(model, 0.001)\n",
        "  for ep in range(1, epochs+1):\n",
        "    ids = np.arange(consider)\n",
        "    np.random.shuffle(ids)\n",
        "    start, end = 0, batch_size\n",
        "    while end > start:\n",
        "      x, y = X[ids[start:end]], Y[ids[start:end]]\n",
        "      sgd.optimize(x, y)\n",
        "      start, end = end, min(consider, end+batch_size)\n",
        "    err = loss_func.error(X,Y,model)\n",
        "    ##print(f'epoch: {ep}, error: {err}')\n",
        "  Y_pred=loss_func.predict(X,Y,model)\n",
        "  print(Y_pred)\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d9z3nLHrSMU",
        "outputId": "9fb4c91c-81ee-4abd-dd6a-e9f0d3b76b8c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy is:12.0\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    }
  ]
}