{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c9a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import tensorflow.keras.preprocessing.image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a948d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 790141528983381638\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2258055988\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9554428634379386776\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    pass\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe618494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb4ffac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"E:\\deep learning project\\inaturalist_12K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea7cd516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path,augmentation=True):\n",
    "    train_path=os.path.join(path,\"train\")\n",
    "    test_path=os.path.join(path,\"val\")\n",
    "    if augmentation==False:\n",
    "        train_generator=tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,validation_split=0.1)\n",
    "    else:\n",
    "        train_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                          rotation_range=90,\n",
    "                                          zoom_range=0.2,\n",
    "                                          shear_range=0.2,\n",
    "                                          validation_split=0.1,\n",
    "                                          horizontal_flip=True)\n",
    "    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "    train_data = train_generator.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=(256, 256),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42)\n",
    "    valid_data=train_generator.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=(256, 256),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42)\n",
    "    test_data=test_generator.flow_from_directory(\n",
    "    directory=test_path,\n",
    "    target_size=(256, 256),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42)\n",
    "    return train_data,valid_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bbbf231",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=256\n",
    "output_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a2479bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (4279357230.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [12]\u001b[1;36m\u001b[0m\n\u001b[1;33m    os.environ['WANDB_NOTEBOOK_NAME'] = \"C:\\Users\\akash saini\\a2.ipynb\"\u001b[0m\n\u001b[1;37m                                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bb6641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51de135c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "694f3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.keras import WandbCallback\n",
    "class CNN(object):\n",
    "    def __init__(self,no_conv_layers,kernel_size,learning_rate,epochs,padding,filter_no_metric,dense_layer_size=32,activation_func='relu',\n",
    "               no_filters=32,image_size=256,drop_out=0.2,output_size=10,\n",
    "               augmentation= True,batch_normalization=True):\n",
    "        self.kenel_size=kernel_size\n",
    "        self.learning_rate=learning_rate\n",
    "        self.image_size=image_size\n",
    "        self.output_size=output_size\n",
    "        self.augmentation=augmentation\n",
    "        self.batch_normalization=batch_normalization\n",
    "        self.no_conv_layers=no_conv_layers\n",
    "        self.dense_layer_size=dense_layer_size\n",
    "        self.drop_out=drop_out\n",
    "        self.no_filters=no_filters\n",
    "        self.epochs=epochs\n",
    "        self.padding=padding\n",
    "        self.filter_no_metric= filter_no_metric\n",
    "        self.activation_func=activation_func\n",
    "        self.initialize(no_conv_layers,kernel_size,learning_rate,epochs,padding,filter_no_metric,dense_layer_size,activation_func,\n",
    "                    no_filters,image_size,drop_out,output_size,\n",
    "                    augmentation,batch_normalization)\n",
    "    def initialize(self,no_conv_layers,kernel_size,learning_rate,epochs,padding,filter_no_metric,dense_layer_size=32,activation_func='relu',\n",
    "                 no_filters=32,image_size=256,drop_out=0.2,output_size=10,\n",
    "                    augmentation= True,batch_normalization=True\n",
    "                 ):\n",
    "        self.model=Sequential()\n",
    "        print(activation_func)\n",
    "        for i in range(0,no_conv_layers):\n",
    "            if i==0:\n",
    "                self.model.add(Conv2D(no_filters,kernel_size, input_shape=(image_size, image_size, 3),kernel_initializer = \"he_uniform\",padding = padding,\n",
    "                           data_format=\"channels_last\"))\n",
    "            else:\n",
    "                if filter_no_metric==\"1\":\n",
    "                    self.model.add(Conv2D(no_filters,kernel_size,kernel_initializer = \"he_uniform\",padding =padding))\n",
    "                elif filter_no_metric==\"2\":\n",
    "                    self.model.add(Conv2D(no_filters*(2**i),kernel_size,kernel_initializer = \"he_uniform\",\n",
    "                                          padding =padding))\n",
    "                elif filter_no_metric==\"1/2\":\n",
    "                    self.model.add(Conv2D(no_filters*(1/(2**i)),kernel_size,kernel_initializer = \"he_uniform\",\n",
    "                                          padding =padding))\n",
    "\n",
    "            self.model.add(Activation(activation_func))\n",
    "            if batch_normalization==True:\n",
    "                self.model.add(BatchNormalization())\n",
    "            self.model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        if batch_normalization==True:\n",
    "            self.model.add(BatchNormalization())\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(dense_layer_size))\n",
    "        self.model.add(Activation(activation_func))\n",
    "        if batch_normalization==True:\n",
    "            self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(drop_out))\n",
    "        self.model.add(Dense(output_size))\n",
    "        self.model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "np.random.seed(2)\n",
    "classes=[\"Amphibia\",\"Animalia\",\"Arachnida\",\"Aves\",\"Fungi\",\"Insecta\",\"Mammalia\",\"Mollusca\",\"Plantae\",\"Reptilia\"]\n",
    "train_data,valid_data,test_data= get_data(path,True)\n",
    "best_model=CNN(5,(3,3),0.001,1,'same','1',64,'elu',\n",
    "              32,image_size,0.4,output_size,True,True)\n",
    "best_model.model.compile(\n",
    "optimizer=Adam(0.001),  # Optimizer\n",
    "loss=\"categorical_crossentropy\", metrics=\"categorical_accuracy\")\n",
    "  #train_data,valid_data,test_data=get_data(path,augmentation=True)\n",
    "best_model.model.fit(train_data,epochs=1,batch_size=32,validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109baf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5991c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d652cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path=os.path.join(path,\"val\")\n",
    "img_path=os.path.join(test_path,\"Amphibia\")\n",
    "lst=os.listdir(img_path)\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    image = cv2.imread(os.path.join(img_path,lst[0]))\n",
    "    image=cv2.resize(image,(256,256))\n",
    "    image=image/255\n",
    "    plt.imshow(image)\n",
    "    pred=best_model.model.predict(image)\n",
    "    print(pred)\n",
    "except:\n",
    "    print(\"not _read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b899f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def test(model,test_path,test_data,cls,classes):\n",
    "    global k\n",
    "    k=1\n",
    "    img_path=os.path.join(test_path,cls)\n",
    "    lst=os.listdir(img_path)\n",
    "    i=0\n",
    "    fig=plt.figure(figsize=(20,30))\n",
    "    for img in lst:\n",
    "        if i!=3:\n",
    "            try:\n",
    "                image = cv2.imread(os.path.join(img_path,img))\n",
    "                image=cv2.resize(image,(image_size,image_size))\n",
    "                fig.add_subplot(10,3,k)\n",
    "                plt.imshow(image)\n",
    "                plt.axis(\"off\")\n",
    "                image=image/255\n",
    "                pred=model.predict(image.reshape(1,image_size,image_size,3))\n",
    "                c=pred.argmax()\n",
    "                plt.title(\"True_label-\"+cls+\"\\n\"+\"pred_label-\"+classes[c])\n",
    "                i=i+1\n",
    "                k=k+1\n",
    "            except:\n",
    "                break\n",
    "        elif i==3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff066dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls in classes:\n",
    "    test(best_model.model,test_path,test_data,cls,classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5c88a",
   "metadata": {},
   "source": [
    "# filter visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.model.evaluate(test_data, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8612246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "import random\n",
    "def plot_first_layer_filter(test_data,classes,model,layer='conv2d'):\n",
    "    outputs =model.get_layer(layer).output\n",
    "    inputs=model.inputs\n",
    "    filter_maps_output=Model(inputs=inputs,outputs=outputs)\n",
    "    length=len(classes)\n",
    "    n_2=random.randrange(test_data[0][0].shape[0])\n",
    "    n_3=random.randrange(test_data[0][0].shape[0])\n",
    "    filter_1=filter_maps_output(test_data[n_2][0])\n",
    "    plt.imshow(test_data[n_2][0][n_3])\n",
    "    plt.axis(\"off\")\n",
    "    wandb.init(project=\"filter\", entity=\"cs21m003_cs21d406\")\n",
    "    wandb.log({\"true_image\":plt})\n",
    "    no_filters=filter_1.shape[3]\n",
    "    figsize=(30,30)\n",
    "    fig=plt.figure(figsize=figsize)\n",
    "    k=1\n",
    "    rows = 8\n",
    "    columns=4\n",
    "    for i in range(no_filters):\n",
    "        fig.add_subplot(rows,columns,k)\n",
    "        plt.imshow(filter_1[n_3,:,:,i])\n",
    "        plt.title(str(i+1)+\"_filter\")\n",
    "        plt.axis(\"off\")\n",
    "        k=k+1\n",
    "    wandb.log({\"filters\":plt})\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c09392",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_first_layer_filter(test_data,classes,best_model.model,'conv2d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548850a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.model.summary()\n",
    "outputs = [best_model.model.get_layer('conv2d').output]\n",
    "inputs=best_model.model.inputs\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "fil_model = Model(\n",
    "      inputs = inputs,    \n",
    "      outputs = outputs )\n",
    "print(test_data[2][0].shape)\n",
    "feature_maps = fil_model(test_data[0][0])\n",
    "plt.imshow(feature_maps[2,:,:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dcebcf",
   "metadata": {},
   "source": [
    "# guided backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa9c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6abee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: rjqhyl4o\n",
      "Sweep URL: https://wandb.ai/cs21m003_cs21d406/random/sweeps/rjqhyl4o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hwpnjrkj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tact_func: elu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_normalization: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_out: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_no_metric: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [5, 5]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: valid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttype: random\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find akash.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\akash saini\\wandb\\run-20220326_075427-hwpnjrkj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cs21m003_cs21d406/random/runs/hwpnjrkj\" target=\"_blank\">iconic-sweep-1</a></strong> to <a href=\"https://wandb.ai/cs21m003_cs21d406/random\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs21m003_cs21d406/random/sweeps/rjqhyl4o\" target=\"_blank\">https://wandb.ai/cs21m003_cs21d406/random/sweeps/rjqhyl4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9999 images belonging to 10 classes.\n",
      "Found 9999 images belonging to 10 classes.\n",
      "Found 2000 images belonging to 10 classes.\n",
      "elu\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 1058s 3s/step - loss: 2.4580 - categorical_accuracy: 0.1974 - val_loss: 2.1241 - val_categorical_accuracy: 0.2484\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 672s 2s/step - loss: 2.1601 - categorical_accuracy: 0.2551 - val_loss: 1.9655 - val_categorical_accuracy: 0.3160\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 650s 2s/step - loss: 2.0176 - categorical_accuracy: 0.2910 - val_loss: 1.8742 - val_categorical_accuracy: 0.3341\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 489s 2s/step - loss: 1.9302 - categorical_accuracy: 0.3194 - val_loss: 1.9945 - val_categorical_accuracy: 0.2992\n",
      "Epoch 5/10\n",
      "120/313 [==========>...................] - ETA: 1:23 - loss: 1.8671 - categorical_accuracy: 0.3375"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "np.random.seed(2)\n",
    "class Runner(object):\n",
    "    def __init__(self,inaturalist_data=True,augmentation=True):\n",
    "        if inaturalist_data:\n",
    "            self.initialize_inaturalist_data(augmentation)\n",
    "        else:\n",
    "            self.initialize_data()\n",
    "\n",
    "    def initialize_inaturalist_data(self,augmentation=True):\n",
    "        self.train_data,self.valid_data,self.test_data= get_data(path,augmentation)\n",
    "    \n",
    "\n",
    "    def initialize_data(self):\n",
    "        raise NotImplementedError(\"Please implement this method if you need other dataset.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_activation_function(key):\n",
    "        mapper = {\n",
    "      #\"sigmoid\": Sigmoid,\n",
    "      \"relu\": 'relu'\n",
    "      #\"tanh\": Tanh\n",
    "    }\n",
    "        assert key in mapper\n",
    "        return mapper[key]\n",
    "\n",
    "def run_wandb():\n",
    "    wandb.init(project=\"random\", entity=\"cs21m003_cs21d406\")\n",
    "    config = wandb.config\n",
    "    wandb.run.name=f\"e_{config.epochs}_bs_{config.batch_size}_kernel_size_{config.kernel_size}_filters_{config.no_filters}_ac_{config.act_func}_rate_{config.learning_rate}_aug_{config.augmentation}_BN_{config.batch_normalization}_drp_{config.drop_out}_pad_{config.padding}_dense_{config.dense_size}_metric_{config.filter_no_metric}_type_{config.type}\"\n",
    "    runner=Runner(True,config.augmentation)\n",
    "    \"\"\"\n",
    "    params = {\n",
    "    \"epochs\"        : config.epochs,\n",
    "    \"batch_size\"    : config.batch_size,\n",
    "    \"kernel_size\"   : config.kernel_size,\n",
    "    \"no_filters\"    : config.no_filters,\n",
    "    \"act_func\"      : config.act_func,\n",
    "    \"learning_rate\" : config.learning_rate,\n",
    "    \"augmentation\"  : config.augmentation,\n",
    "    \"batch_normalization\"  : config.batch_normalization,\n",
    "    \"drop_out\"  : config.drop_out,\n",
    "    \"padding\": config.padding,\n",
    "    \"dense_size\":config.dense_size,\n",
    "    \"filter_no_metric\":config.filter_no_metric\n",
    "\n",
    "    }\n",
    "    \"\"\"\n",
    "  #no_conv_layers,filter_size,learning_rate,epochs,padding,filter_size_metric,dense_layer_size=32,activation_func='relu',\n",
    "              # no_filters=32,image_size=256,drop_out=0.2,output_size=10,\n",
    "               #augmentation= True,batch_normalization=True)\n",
    "    model_1=CNN(5,config.kernel_size,config.learning_rate,config.epochs,config.padding,config.filter_no_metric,config.dense_size,config.act_func,\n",
    "              config.no_filters,image_size,config.drop_out,output_size,config.augmentation,config.batch_normalization)\n",
    "    model_1.model.compile(\n",
    "    optimizer=Adam(config.learning_rate),  # Optimizer\n",
    "    loss=\"categorical_crossentropy\", metrics=\"categorical_accuracy\")\n",
    "  #train_data,valid_data,test_data=get_data(path,augmentation=True)\n",
    "    model_1.model.fit(runner.train_data,epochs=config.epochs,batch_size=config.batch_size,validation_data=runner.valid_data,\n",
    "          callbacks=[WandbCallback()])\n",
    "    loss,accuracy=model_1.model.evaluate(runner.test_data,batch_size=config.batch_size)\n",
    "    print(f'test accuracy:{accuracy}')\n",
    "    wandb.log({\"test accuracy\":accuracy})\n",
    "\n",
    "def do_hyperparameter_search_using_wandb():\n",
    "    sweep_config = {\n",
    "    \"name\": \"random sweep\",\n",
    "    \"method\": \"random\",\n",
    "    \"metric\":{\n",
    "      \"name\": \"ValidationAccuracy\",\n",
    "      \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "      \"type\": {\"values\": ['random']},\n",
    "      \"epochs\": {\"values\": [5, 10]}, \n",
    "      \"batch_size\": {\"values\": [32]}, \n",
    "      \"kernel_size\": {\"values\": [(4,4),(5,5)]}, \n",
    "      \"no_filters\": {\"values\": [32,64]},\n",
    "      \"act_func\": {\"values\": ['elu', 'selu']}, \n",
    "      \"learning_rate\": {\"values\": [1e-3, 1e-4]}, \n",
    "      \"augmentation\": {\"values\": [True,False]} , \n",
    "      \"batch_normalization\": {\"values\": [True,False]},\n",
    "      \"drop_out\": {\"values\": [0.2,0.3,0.4]},\n",
    "      \"padding\": {\"values\": ['same','valid']},\n",
    "      \"dense_size\": {\"values\": [32,64]},\n",
    "      \"filter_no_metric\": {\"values\": [\"1\",\"2\",\"1/2\"]}}}\n",
    "  \n",
    "    sweep_id = wandb.sweep(sweep_config, project = \"random\",entity='cs21m003_cs21d406')\n",
    "    wandb.agent(sweep_id, function=run_wandb,count=10)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    do_hyperparameter_search_using_wandb()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72072ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "np.random.seed(2)\n",
    "classes=[\"Amphibia\",\"Animalia\",\"Arachnida\",\"Aves\",\"Fungi\",\"Insecta\",\"Mammalia\",\"Mollusca\",\"Plantae\",\"Reptilia\"]\n",
    "train_data,valid_data,test_data= get_data(path,True)\n",
    "best_model=CNN(5,(3,3),0.001,1,'same','1',64,'elu',\n",
    "              128,image_size,0.4,output_size,True,True)\n",
    "best_model.model.compile(\n",
    "optimizer=Adam(0.001),  # Optimizer\n",
    "loss=\"categorical_crossentropy\", metrics=\"categorical_accuracy\")\n",
    "  #train_data,valid_data,test_data=get_data(path,augmentation=True)\n",
    "best_model.model.fit(train_data,epochs=1,batch_size=32,validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8bf856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
