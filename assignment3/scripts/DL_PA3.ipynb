{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_PA3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "m1ASDFF71FfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d513306f-a95e-4124-e768-003adb333fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3avo5OSVt5rK",
        "outputId": "41306446-f4b7-48cd-9987-6e9d16e1bdad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.16)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.11)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQkdn3RVt_nX",
        "outputId": "25ba8448-314c-4551-f631-fe50442800ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreturaj\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHK3kKHUSeJl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(0)\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "AtZPZbPqZw96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(object):\n",
        "  def __init__(self, data_path):\n",
        "    self.train_path = os.path.join(data_path, 'hi.translit.sampled.train.tsv')\n",
        "    self.validation_path = os.path.join(data_path, 'hi.translit.sampled.dev.tsv')\n",
        "    self.test_path = os.path.join(data_path, 'hi.translit.sampled.test.tsv')\n",
        "    self.encoder_tokenizer = None\n",
        "    self.decoder_tokenizer = None\n",
        "    self.load_train_data = False\n",
        "\n",
        "  @staticmethod\n",
        "  def _read_file(filepath):\n",
        "    encoder_words, decoder_words = [], []\n",
        "    with open(filepath, 'r') as fp:\n",
        "      for line in fp:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "          continue\n",
        "        target, src, _ = [x.strip() for x in line.split('\\t')]\n",
        "        src = src + \"\\n\"  # \\n represents end_of_word\n",
        "        encoder_words.append(src)\n",
        "        target = \"\\t\" + target + \"\\n\"  # \\t represents start_word and \\n represents end_of_word\n",
        "        decoder_words.append(target)\n",
        "    return encoder_words, decoder_words\n",
        "\n",
        "  @property\n",
        "  def vocab_size(self):\n",
        "    assert self.load_train_data, \"Seems like you want to know the vocab size even before loading train data\"\n",
        "    encoder_vocab_size = len(self.encoder_tokenizer.word_index) + 1 # number 0 is reserved for padding\n",
        "    decoder_vocab_size = len(self.decoder_tokenizer.word_index) + 1 # number 0 is reserved for padding\n",
        "    return encoder_vocab_size, decoder_vocab_size\n",
        "\n",
        "  def _reset_tokenizer(self):\n",
        "    self.load_train_data = False\n",
        "    self.encoder_tokenizer = None\n",
        "    self.decoder_tokenizer = None\n",
        "\n",
        "  def _get_tokenizer(self, encoder_words, decoder_words):\n",
        "    assert self.load_train_data, \"Seems like you are trying to access test data even before accessing train data !!\"\n",
        "    if self.encoder_tokenizer is None:\n",
        "      self.encoder_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "      self.encoder_tokenizer.fit_on_texts(encoder_words)\n",
        "    if self.decoder_tokenizer is None:\n",
        "      self.decoder_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "      self.decoder_tokenizer.fit_on_texts(decoder_words) \n",
        "    return self.encoder_tokenizer, self.decoder_tokenizer\n",
        "\n",
        "  def _get_dataset(self, encoder_words, decoder_words):\n",
        "    encoder_tokenizer, decoder_tokenizer = self._get_tokenizer(encoder_words, decoder_words)\n",
        "    encoder_input = encoder_tokenizer.texts_to_sequences(encoder_words)\n",
        "    encoder_input = tf.keras.preprocessing.sequence.pad_sequences(encoder_input, padding='post') \n",
        "    decoder_target = decoder_tokenizer.texts_to_sequences(decoder_words)\n",
        "    decoder_target = tf.keras.preprocessing.sequence.pad_sequences(decoder_target, padding='post')\n",
        "    return encoder_input, decoder_target\n",
        "\n",
        "  def get_training_data(self):\n",
        "    try:\n",
        "      self.load_train_data = True\n",
        "      train_encoder_words, train_decoder_words = self._read_file(self.train_path)\n",
        "      train_encoder_input, train_decoder_target = self._get_dataset(train_encoder_words, train_decoder_words)\n",
        "      val_encoder_words, val_decoder_words = self._read_file(self.validation_path)\n",
        "      val_encoder_input, val_decoder_target = self._get_dataset(val_encoder_words, val_decoder_words)\n",
        "    except Exception as ex:\n",
        "      self._reset_tokenizer()\n",
        "      raise ex\n",
        "    return train_encoder_input, train_decoder_target, val_encoder_input, val_decoder_target\n",
        "\n",
        "  def get_testing_data(self):\n",
        "    test_encoder_words, test_decoder_words = self._read_file(self.test_path)\n",
        "    test_encoder_input, _, test_decoder_target = self._get_dataset(test_encoder_words, test_decoder_words)\n",
        "    return test_encoder_input, test_decoder_target"
      ],
      "metadata": {
        "id": "d3_0odxVNsjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Experiments"
      ],
      "metadata": {
        "id": "gzaKE8TRk1M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(latent_dim)\n",
        "    self.W2 = tf.keras.layers.Dense(latent_dim)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, decoder_state, encoder_output):\n",
        "    decoder_state = tf.concat(decoder_state, 1)\n",
        "    decoder_state = tf.expand_dims(decoder_state, 1)\n",
        "    score = self.V(tf.nn.tanh(self.W1(decoder_state) + self.W2(encoder_output)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * encoder_output\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "pZgMn3SpHtwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseModel(tf.keras.Model):\n",
        "  def __init__(self, params, rnn_class):\n",
        "    super(BaseModel, self).__init__()\n",
        "    self.set_attributes(params)\n",
        "\n",
        "  def set_attributes(self, params):\n",
        "    for k, v in params.items():\n",
        "      setattr(self, k, v)\n",
        "\n",
        "  def stacked_layers(self, rnn_class, num_layers):\n",
        "    first_rnn = rnn_class(self.latent_dim, return_state=True, return_sequences=True)\n",
        "    if num_layers <= 1:\n",
        "      return first_rnn, None\n",
        "    stacked_input = tf.keras.Input(shape=(None, self.latent_dim))\n",
        "    stacked_ouput = stacked_input\n",
        "    for layer in range(1, num_layers):\n",
        "      stacked_output = tf.keras.layers.Dropout(self.dropout)(stacked_ouput)\n",
        "      stacked_encoder = rnn_class(self.latent_dim, return_state=True, return_sequences=True)\n",
        "      x = stacked_encoder(stacked_output)\n",
        "      stacked_output = x[0]\n",
        "    stacked_rnn = tf.keras.Model(stacked_input, x)\n",
        "    return first_rnn, stacked_rnn\n",
        "\n",
        "  def call(self, *args, **kwargs):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def initialize_hidden_state(self, batch=None):\n",
        "    if batch == None:\n",
        "      batch = self.batch_size\n",
        "    init = [tf.zeros((batch, self.latent_dim))]\n",
        "    if isinstance(self.first_rnn, tf.keras.layers.LSTM):\n",
        "      init *= 2\n",
        "    return init\n",
        "\n",
        "\n",
        "class Encoder(BaseModel):\n",
        "  def __init__(self, params, rnn_class):\n",
        "    super(Encoder, self).__init__(params, rnn_class)\n",
        "    self.embed = tf.keras.layers.Embedding(self.encoder_vocab_size, self.embed_size, mask_zero=True)\n",
        "    self.first_rnn, self.stacked_rnn = self.stacked_layers(rnn_class, self.num_encoder_layers)\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embed(x)\n",
        "    x = self.first_rnn(x, initial_state=hidden)\n",
        "    if self.num_encoder_layers > 1:\n",
        "      x = self.stacked_rnn(x[0])\n",
        "    output, state = x[0], x[1:]\n",
        "    return (output, state)\n",
        "\n",
        "\n",
        "class Decoder(BaseModel):\n",
        "  def __init__(self, params, rnn_class):\n",
        "    super(Decoder, self).__init__(params, rnn_class)\n",
        "    self.first_rnn, self.stacked_rnn = self.stacked_layers(rnn_class, self.num_decoder_layers)\n",
        "    self.dense = tf.keras.layers.Dense(self.decoder_vocab_size, activation=\"softmax\")\n",
        "    if self.use_attention:\n",
        "      self.attention = Attention(self.latent_dim)\n",
        "\n",
        "  def call(self, x, hidden, encoder_output=None):\n",
        "    x = tf.one_hot(x, depth=self.decoder_vocab_size)\n",
        "    attention_weights = None\n",
        "    if self.use_attention:\n",
        "      context_vector, attention_weights = self.attention(hidden, encoder_output)\n",
        "      x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    x = self.first_rnn(x, initial_state=hidden)\n",
        "    if self.num_decoder_layers > 1:\n",
        "      x = self.stacked_rnn(x[0])\n",
        "    output, state = x[0], x[1:]\n",
        "    output = self.dense(output)\n",
        "    return (output, state, attention_weights)"
      ],
      "metadata": {
        "id": "yrBsfQJScztR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Runner(object):\n",
        "  def __init__(self, params, rnn_class, encoder_tokenizer, decoder_tokenizer):\n",
        "    self.params = params\n",
        "    self.encoder_tokenizer = encoder_tokenizer\n",
        "    self.decoder_tokenizer = decoder_tokenizer\n",
        "    self.encoder = Encoder(params, rnn_class)\n",
        "    self.decoder = Decoder(params, rnn_class)\n",
        "    self.optimizer = tf.keras.optimizers.Adam()\n",
        "    self.loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "  @staticmethod\n",
        "  def index_word(tokenizer, seq):\n",
        "    result = ''\n",
        "    for s in seq:\n",
        "      if s == 0: # generally we should not encounter this id, but it we do then it is just a unrecognized character\n",
        "        result += '?'\n",
        "      else:\n",
        "        result += tokenizer.index_word[s]\n",
        "      if result[-1] == '\\n':\n",
        "        break\n",
        "    return result\n",
        "\n",
        "  @staticmethod\n",
        "  def word_index(tokenizer, seq, max_length):\n",
        "    result = []\n",
        "    for s in seq:\n",
        "      result.append(tokenizer.word_index[s])\n",
        "    result = result + [0] *(max_length - len(result))\n",
        "    return np.array(result)\n",
        "\n",
        "  def _custom_loss_function(self, real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0)) # finds all the dummy characters that were added to make the sequcence length equal across data\n",
        "    loss = self.loss_obj(real, pred) # returns the cross entropy for each data\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask # removes all the dummy characters from loss calculation\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "  @tf.function\n",
        "  def _train_step(self, encoder_input, decoder_target):\n",
        "    loss = 0\n",
        "    encoder_hidden = self.encoder.initialize_hidden_state(batch=encoder_input.shape[0])\n",
        "    with tf.GradientTape() as tape:\n",
        "      encoder_output, encoder_hidden = self.encoder(encoder_input, encoder_hidden)\n",
        "      decoder_hidden = encoder_hidden\n",
        "      decoder_input = tf.expand_dims(decoder_target[:, 0], 1)\n",
        "      for t in range(1, decoder_target.shape[1]):  # unfolding in time\n",
        "        pred_prob, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "        loss += self._custom_loss_function(decoder_target[:, t], pred_prob)\n",
        "        decoder_input = tf.expand_dims(decoder_target[:, t], 1)\n",
        "    batch_loss = loss / int(decoder_target.shape[1])  # normalizing in time\n",
        "    trainable_variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "    grads = tape.gradient(loss, trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(grads, trainable_variables))\n",
        "    return batch_loss\n",
        "\n",
        "  def train(self, encoder_input, decoder_target, val_encoder_input, val_decoder_target, epochs=5):\n",
        "    num_train_data = encoder_input.shape[0]\n",
        "    indx = np.arange(num_train_data)\n",
        "    np.random.shuffle(indx)\n",
        "    train_loss, valid_accuracy = [], []\n",
        "    for epoch in range(epochs):\n",
        "      total_loss = 0\n",
        "      step = 0\n",
        "      start, end = 0, self.params['batch_size']\n",
        "      while start < num_train_data:\n",
        "        batch_indx = indx[start:end]\n",
        "        inp, targ = encoder_input[batch_indx, :], decoder_target[batch_indx, :]\n",
        "        total_loss += self._train_step(inp, targ)\n",
        "        start = end\n",
        "        end += self.params['batch_size']\n",
        "        step += 1\n",
        "      val_acc = self.validation_step(val_encoder_input, val_decoder_target)\n",
        "      train_loss.append(total_loss/step)\n",
        "      valid_accuracy.append(val_acc)\n",
        "      # comment this line if you don't want to print loss/acc\n",
        "      # print(f\"Epoch: {epoch+1}, Loss: {total_loss/step}, val_acc: {val_acc}\")\n",
        "    return train_loss, valid_accuracy\n",
        "\n",
        "  def translate(self, encoder_input, max_target_len):\n",
        "    batch = encoder_input.shape[0]\n",
        "    encoder_hidden = self.encoder.initialize_hidden_state(batch)\n",
        "    encoder_output, decoder_hidden = self.encoder(encoder_input, encoder_hidden)\n",
        "    result = np.zeros((batch, max_target_len), dtype=int)\n",
        "    result[:, 0] = self.decoder_tokenizer.word_index['\\t']\n",
        "    decoder_input = tf.expand_dims(result[:, 0], 1)\n",
        "    for t in range(1, max_target_len):\n",
        "      pred_prob, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "      pred_id = tf.argmax(pred_prob, -1)\n",
        "      result[:, t] = pred_id[:, 0]\n",
        "      decoder_input = pred_id\n",
        "    return result\n",
        "\n",
        "  def validation_step(self, encoder_input, decoder_target):\n",
        "    max_target_len = decoder_target.shape[1]\n",
        "    results = self.translate(encoder_input, max_target_len)\n",
        "    val_accuracy = 0\n",
        "    for r, t in zip(results, decoder_target):\n",
        "      res_word = self.index_word(self.decoder_tokenizer, r)\n",
        "      targ_word = self.index_word(self.decoder_tokenizer, t)\n",
        "      val_accuracy += 1 if res_word == targ_word else 0\n",
        "    val_accuracy /= decoder_target.shape[0]\n",
        "    return val_accuracy"
      ],
      "metadata": {
        "id": "M2mskhoLhKb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In most likely situation you only need to change this cell data if you need to, if you need to change anything else go ahead and change\n",
        "\n",
        "\n",
        "RNN_MAP = {\n",
        "    \"lstm\": tf.keras.layers.LSTM,\n",
        "    \"gru\": tf.keras.layers.GRU,\n",
        "    \"rnn\": tf.keras.layers.SimpleRNN\n",
        "}\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/IITM/collab/cs6910/RNN_data_set/dakshina_dataset_v1.0/hi/lexicons'\n",
        "\n",
        "WANDB_PROJECT = \"CS6910_ASSIGNMENT_3\"\n",
        "WANDB_ENTITY = \"cs21m003_cs21d406\"\n",
        "WANDB_RUNS = 20\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "\n",
        "class WandbRunner(object):\n",
        "  def __init__(self):\n",
        "    dataset = Dataset(DATA_PATH)\n",
        "    self.train_encoder_input, self.train_decoder_target, self.val_encoder_input, self.val_decoder_target = dataset.get_training_data()\n",
        "    self.encoder_vocab_size, self.decoder_vocab_size = dataset.vocab_size\n",
        "    self.encoder_tokenizer = dataset.encoder_tokenizer\n",
        "    self.decoder_tokenizer = dataset.decoder_tokenizer\n",
        "\n",
        "  def run_wandb(self):\n",
        "    wandb.init()\n",
        "    config = wandb.config\n",
        "    params = {\n",
        "      \"encoder_vocab_size\": self.encoder_vocab_size,\n",
        "      \"decoder_vocab_size\": self.decoder_vocab_size, \n",
        "      \"embed_size\": config.inp_embed_size,\n",
        "      \"latent_dim\": config.latent_dim,\n",
        "      \"num_encoder_layers\": config.num_encoder_layers,\n",
        "      \"num_decoder_layers\": config.num_decoder_layers,\n",
        "      \"dropout\": config.dropout,\n",
        "      \"batch_size\": config.batch_size, \n",
        "      \"use_attention\": config.attention\n",
        "    }\n",
        "    rnn_class = RNN_MAP[config.rnn_type]\n",
        "    runner = Runner(params, rnn_class, self.encoder_tokenizer, self.decoder_tokenizer)\n",
        "    train_loss, valid_accuracy = runner.train(self.train_encoder_input, self.train_decoder_target,\n",
        "                                              self.val_encoder_input, self.val_decoder_target, epochs=config.epochs)\n",
        "    wandb.run.name=f\"emb_{config.inp_embed_size}_ld_{config.latent_dim}_nel_{config.num_encoder_layers}_ndl_{config.num_decoder_layers}_dpt_{config.dropout}_at_{config.attention}_bs_{config.batch_size}_cell_{config.rnn_type}\"\n",
        "    for tl, va in zip(train_loss, valid_accuracy):\n",
        "      wandb.log({\"training_loss\": tl, \"validation_accuracy\": va})\n",
        "\n",
        "  def do_hyperparameter_search(self):\n",
        "    sweep_config = {\n",
        "        \"name\": \"Transliteration Search\",\n",
        "        \"method\": \"random\",\n",
        "        \"metric\": {\n",
        "            \"name\": \"validation_accuracy\",\n",
        "            \"goal\": \"maximize\"\n",
        "        },\n",
        "        \"parameters\": {\n",
        "            \"inp_embed_size\": {\"values\": [16, 32]}, # run for [64, 256] later\n",
        "            \"latent_dim\": {\"values\": [16, 32]}, # run for [64, 256] later\n",
        "            \"num_encoder_layers\": {\"values\": [1, 2, 3]},\n",
        "            \"num_decoder_layers\": {\"values\": [1, 2, 3]},\n",
        "            \"dropout\": {\"values\": [0.2, 0.3, 0.4]},\n",
        "            \"batch_size\": {\"values\": [32, 64]},\n",
        "            \"attention\": {\"values\": [False]},  # Donot change this for question 1\n",
        "            \"rnn_type\": {\"values\": [\"lstm\", \"gru\"]}, # run for rnn later\n",
        "            \"epochs\": {\"values\": [EPOCHS]}\n",
        "        }\n",
        "    }\n",
        "    sweep_id = wandb.sweep(sweep_config, project=WANDB_PROJECT, entity=WANDB_ENTITY)\n",
        "    wandb.agent(sweep_id, function=self.run_wandb, count=WANDB_RUNS)"
      ],
      "metadata": {
        "id": "0QOOTTpmJoyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code to call wandb\n",
        "# It takes about 15-20 mins to complete 1 run, so be patient\n",
        "\n",
        "wandb_runner = WandbRunner()\n",
        "wandb_runner.do_hyperparameter_search()"
      ],
      "metadata": {
        "id": "wdZrVdxyJovb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tiKVCSeBJicT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Do not run this, it is only for testing purposes\n",
        "\n",
        "# dataset = Dataset(DATA_PATH)\n",
        "# train_encoder_input, train_decoder_target, val_encoder_input, val_decoder_target = dataset.get_training_data()\n",
        "# encoder_vocab_size, decoder_vocab_size = dataset.vocab_size\n",
        "\n",
        "# params = {\n",
        "#   \"encoder_vocab_size\": encoder_vocab_size,\n",
        "#   \"decoder_vocab_size\": decoder_vocab_size, \n",
        "#   \"embed_size\": 30,\n",
        "#   \"latent_dim\": 30,\n",
        "#   \"num_encoder_layers\": 2,\n",
        "#   \"num_decoder_layers\": 2,\n",
        "#   \"dropout\": 0.2,\n",
        "#   \"batch_size\": 32, \n",
        "#   \"use_attention\": False\n",
        "# }\n",
        "\n",
        "# run = Runner(params, RNN_MAP['rnn'], dataset.encoder_tokenizer, dataset.decoder_tokenizer)\n",
        "# tl, va = run.train(train_encoder_input, train_decoder_target, val_encoder_input, val_decoder_target)\n",
        "# print(va)  # prints validation accuracy at each epochs"
      ],
      "metadata": {
        "id": "b3jyqe0ZHIjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AsRUG9kSHIhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QhVgNiuHHIe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "odF56fqqHIco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fil49j_IHIaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# student's assignment:\n",
        "# https://github.com/sujaybokil/CS6910-Assignment3/blob/master/DL_Assignment3_Master.ipynb\n",
        "\n",
        "# Paperspace blog\n",
        "# https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/\n",
        "\n",
        "# stacked lstm codes:\n",
        "# https://github.com/sachinruk/PyData_Keras_Talk/blob/master/cosine_LSTM.ipynb\n",
        "\n",
        "# seq2seq model tf\n",
        "# https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt\n",
        "\n",
        "# masking and padding\n",
        "# https://www.tensorflow.org/guide/keras/masking_and_padding\n",
        "\n",
        "# transformer\n",
        "# https://www.tensorflow.org/text/tutorials/transformer\n",
        "\n",
        "# one hot encoding\n",
        "# https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
        "\n",
        "# masking (can use one hot encoding with masking)\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking\n",
        "\n",
        "# masked loss function\n",
        "# https://stackoverflow.com/questions/56328140/how-do-i-implement-a-masked-softmax-cross-entropy-loss-function-in-keras\n",
        "\n",
        "# char seq2seq lstm\n",
        "# https://keras.io/examples/nlp/lstm_seq2seq/\n"
      ],
      "metadata": {
        "id": "trluXQ1nmn71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JQkp1nYsmn4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DXHZojDemPWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "isRmMBIntzzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g4SVwCLbT1gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8snltR9wT1c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GLSJo7-rT1ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3YT19MXINou4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6P9IiGZxNokF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}