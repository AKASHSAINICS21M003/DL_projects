{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOsbCISWHgiTPW2ISee27Jd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/returaj/cs6910/blob/assginment1_akash/q10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj9bTH2dFBYL",
        "outputId": "9a8ccf75-2894-46da-db84-66f0bbae029f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp drive/MyDrive/DLASSIGNMENT/activation_func.py .\n",
        "import activation_func\n",
        "!cp drive/MyDrive/DLASSIGNMENT/loss_func.py .\n",
        "import loss_func\n",
        "!cp drive/MyDrive/DLASSIGNMENT/runner.py .\n",
        "!cp drive/MyDrive/DLASSIGNMENT/feed_forward_nn.py .\n",
        "!cp drive/MyDrive/DLASSIGNMENT/measure.py .\n",
        "!cp drive/MyDrive/DLASSIGNMENT/optimizer.py .\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "\n",
        "from feed_forward_nn import FNN\n",
        "from measure import accuracy\n",
        "from optimizer import SGD, MomentumGD, NesterovGD, Rmsprop, Adam, Nadam\n",
        "from activation_func import Sigmoid, Relu, Tanh\n",
        "from loss_func import CrossEntropy, MeanSquaredError\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from feed_forward_nn import FNN\n",
        "from measure import accuracy\n",
        "from optimizer import SGD, MomentumGD, NesterovGD, Rmsprop, Adam, Nadam\n",
        "from activation_func import Sigmoid, Relu, Tanh\n",
        "from loss_func import CrossEntropy, MeanSquaredError\n",
        "from sklearn.model_selection import train_test_split\n",
        "#np.random.seed(2)\n"
      ],
      "metadata": {
        "id": "EEiQlnuEFvZ7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class digits_mnist(object):\n",
        "  def __init__(self, MnistData=True):\n",
        "    if MnistData:\n",
        "      self.initialize_mnist_data()\n",
        "    else:\n",
        "      self.initialize_data()\n",
        "\n",
        "  def initialize_mnist_data(self):\n",
        "    (X, self.y), (X_test, self.y_test) = mnist.load_data()\n",
        "    self.X = np.array([x.flatten() for x in X]) / 255\n",
        "    self.X_test = np.array([x.flatten() for x in X_test]) / 255\n",
        "\n",
        "  def initialize_data(self):\n",
        "    raise NotImplementedError(\"Please implement this method if you need other dataset.\")\n",
        "\n",
        "  @staticmethod\n",
        "  def get_loss_function(key):\n",
        "    mapper = {\n",
        "      \"cross_entropy\": CrossEntropy,\n",
        "      \"mse\": MeanSquaredError\n",
        "    }\n",
        "    assert key in mapper\n",
        "    return mapper[key]\n",
        "\n",
        "  @staticmethod\n",
        "  def get_activation_function(key):\n",
        "    mapper = {\n",
        "      \"sigmoid\": Sigmoid,\n",
        "      \"relu\": Relu,\n",
        "      \"tanh\": Tanh\n",
        "    }\n",
        "    assert key in mapper\n",
        "    return mapper[key]\n",
        "\n",
        "  @staticmethod\n",
        "  def get_optimizer(key):\n",
        "    mapper = {\n",
        "      \"sgd\": SGD,\n",
        "      \"momentum_gd\": MomentumGD,\n",
        "      \"nesterov_gd\": NesterovGD,\n",
        "      \"rmsprop\": Rmsprop,\n",
        "      \"adam\": Adam,\n",
        "      \"nadam\": Nadam\n",
        "    }\n",
        "    assert key in mapper\n",
        "    return mapper[key]\n",
        "\n",
        "  @staticmethod\n",
        "  def train(X_train, y_train, params,id,do_val):\n",
        "    \"\"\"\n",
        "    X_train: (batch_size(B), data_size(N))\n",
        "    y_train: (batch_size(B))\n",
        "    params: dict(\n",
        "      batch_size: int,\n",
        "      epochs: int,\n",
        "      alpha: float,\n",
        "      optimizer: One of (SGD, MomentumGD, NesterovGD, Rmsprop, Adam, Nadam),\n",
        "      hidden_layers_size: list(layer_size),\n",
        "      act_func: One of (Sigmoid, Relu, Tanh),\n",
        "      reg: float,\n",
        "      init: One of (random, xavier),\n",
        "      loss_func: One of (CrossEntropy, MeanSquaredError)\n",
        "    )\n",
        "    \"\"\"\n",
        "    print(f\"model:{id}\")\n",
        "    if do_val:\n",
        "      X_train, X_val, y_train, y_val = train_test_split(\n",
        "          X_train, y_train, test_size=0.1, random_state=10)\n",
        "\n",
        "    data_size, input_size = X_train.shape\n",
        "    output_size = 10\n",
        "    batch_size, epochs = params['batch_size'], params['epochs']\n",
        "    act_func, loss_func = params['act_func'](), params['loss_func']()\n",
        "    model = FNN(\n",
        "      input_size         = input_size,\n",
        "      output_size        = output_size,\n",
        "      hidden_layers_size = params['hidden_layers_size'],\n",
        "      act_func           = act_func,\n",
        "      reg                = params['reg'],\n",
        "      init               = params['init'],\n",
        "      loss_func          = loss_func,\n",
        "    )\n",
        "    optimizer = params['optimizer'](model, params['alpha'])\n",
        "    for ep in range(1, epochs+1):\n",
        "      ids = np.arange(data_size)\n",
        "      np.random.shuffle(ids)\n",
        "      start, end = 0, batch_size\n",
        "      while end > start:\n",
        "        x, y = X_train[ids[start:end]], y_train[ids[start:end]]\n",
        "        optimizer.optimize(x, y)\n",
        "        start, end = end, min(data_size, end+batch_size)\n",
        "      # log\n",
        "      train_loss = loss_func.error(X_train, y_train, model)\n",
        "      estimate_y_train = digits_mnist.predict(X_train, model)\n",
        "      train_acc = accuracy(estimate_y_train, y_train)\n",
        "      val_loss, val_acc = \"NotDefined\", \"NotDefined\"\n",
        "      if do_val:\n",
        "        val_loss = loss_func.error(X_val, y_val, model)\n",
        "        estimate_y_val = digits_mnist.predict(X_val, model)\n",
        "        val_acc = accuracy(estimate_y_val, y_val)\n",
        "    digits_mnist.logger(train_loss, train_acc, val_loss, val_acc)\n",
        "    return model\n",
        "\n",
        "  def logger(train_loss, train_acc, val_loss, val_acc):\n",
        "    #print(f\"epochs:{epoch}\")\n",
        "    print(f\"TrainingLoss: {train_loss}, TrainingAccuracy: {train_acc}\")\n",
        "    print(f\"ValidationLoss: {val_loss}, ValidationAccuracy: {val_acc}\")\n",
        "\n",
        "  @staticmethod\n",
        "  def predict(X, model):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    return np.array of size: (batch_size(B),)\n",
        "    \"\"\"\n",
        "    prob = model.forward(X)[-1]\n",
        "    return np.argmax(prob, axis=1)\n",
        "\n",
        "\n",
        "#WANDB_PROJECT = \"Confusion matrix\"\n",
        "#WANDB_ENTITY  = \"cs21m003_cs21d406\"\n",
        "\n",
        "\n",
        "def run(epochs,batch_size,hidden_layers,hidden_nodes,learning_rate,optimizer,act_func,reg,weight_init,id,loss_func):\n",
        "  #wandb.init(project=WANDB_PROJECT, entity=WANDB_ENTITY)\n",
        "  #config = wandb.config\n",
        "  loss_name = \"ce\" if loss_func == \"cross_entropy\" else \"mse\"\n",
        "  #wandb.run.name=f\"e_{config.epochs}_bs_{config.batch_size}_hl_{config.hidden_layers}_hn_{config.hidden_nodes}_init_{config.weight_init}_ac_{config.act_func}_reg_{config.reg}_ls_{loss_name}_opt_{config.optimizer}_lr_{config.learning_rate}_sc_{config.search_type}_best\"\n",
        "  hidden_layers_size = [hidden_nodes] * hidden_layers\n",
        "  runner = digits_mnist()\n",
        "  params = {\n",
        "    \"batch_size\"        : batch_size,\n",
        "    \"epochs\"            : epochs,\n",
        "    \"alpha\"             : learning_rate,\n",
        "    \"optimizer\"         : runner.get_optimizer(optimizer),\n",
        "    \"hidden_layers_size\": hidden_layers_size,\n",
        "    \"act_func\"          : runner.get_activation_function(act_func),\n",
        "    \"reg\"               : reg,\n",
        "    \"init\"              : weight_init,\n",
        "    \"loss_func\"         : runner.get_loss_function(loss_func),\n",
        "  }\n",
        "  model = runner.train(runner.X, runner.y, params,id,do_val=True)\n",
        "  estimate_y_test = runner.predict(runner.X_test, model)\n",
        "  test_acc = accuracy(estimate_y_test, runner.y_test)\n",
        "  print(f'test_accuracy:{test_acc}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  run(5,32,3,128,0.001,\"adam\",\"tanh\",0,'xavier',1,\"cross_entropy\")\n",
        "  run(5,64,3,128,0.001,\"adam\",\"tanh\",0,'xavier',2,\"cross_entropy\")\n",
        "  run(5,32,3,32,0.001,\"nadam\",\"tanh\",0.0005,'xavier',3,\"cross_entropy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVBuJ2pSGBkW",
        "outputId": "d0970cc3-b9aa-490c-d465-4961f546d943"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:1\n",
            "TrainingLoss: 0.05407586550235132, TrainingAccuracy: 0.9832962962962963\n",
            "ValidationLoss: 0.1184817148094781, ValidationAccuracy: 0.9676666666666667\n",
            "test_accuracy:0.9686\n",
            "model:2\n",
            "TrainingLoss: 0.056299638259284614, TrainingAccuracy: 0.9829814814814815\n",
            "ValidationLoss: 0.10671999185562739, ValidationAccuracy: 0.9703333333333334\n",
            "test_accuracy:0.9664\n",
            "model:3\n",
            "TrainingLoss: 0.20367736748791548, TrainingAccuracy: 0.9708148148148148\n",
            "ValidationLoss: 0.2392556843479847, ValidationAccuracy: 0.9606666666666667\n",
            "test_accuracy:0.9602\n"
          ]
        }
      ]
    }
  ]
}