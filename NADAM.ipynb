{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NADAM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMR6hLlbJbEBmZkhaLo+3Qs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/returaj/cs6910/blob/assginment1_akash/NADAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "itl-_iL_7RgZ"
      },
      "outputs": [],
      "source": [
        "class NADAM(object):\n",
        "  def __init__(self, model, alpha,beta1=0.9,beta2=0.99,epsilon=0.0000001):\n",
        "    self.model = model\n",
        "    self.alpha = alpha\n",
        "    self.beta1=beta1\n",
        "    self.beta2=beta2\n",
        "    self.epsilon=epsilon\n",
        "    self.found=0\n",
        "    self.initialize()\n",
        "  \n",
        "  def initialize(self):\n",
        "    self.v_w=[]\n",
        "    self.v_b=[]\n",
        "    self.m_w=[]\n",
        "    self.m_b=[]\n",
        "    self.m_w_hat=[]\n",
        "    self.m_b_hat=[]\n",
        "    self.v_w_hat=[]\n",
        "    self.v_b_hat=[]\n",
        "    self.mw_cap=[]\n",
        "    self.mb_cap=[]\n",
        "    num_layers = len(model.weight)\n",
        "    for i in range(num_layers):\n",
        "      m, n = self.model.weight[i].shape\n",
        "      self.v_w.append(np.zeros((m,n)))\n",
        "      self.v_b.append(np.zeros(n))\n",
        "      self.m_w.append(np.zeros((m,n)))\n",
        "      self.m_b.append(np.zeros(n))\n",
        "      self.m_w_hat.append(np.zeros((m,n)))\n",
        "      self.m_b_hat.append(np.zeros(n))\n",
        "      self.v_w_hat.append(np.zeros((m,n)))\n",
        "      self.v_b_hat.append(np.zeros(n))\n",
        "      self.mw_cap.append(np.zeros((m,n)))\n",
        "      self.mb_cap.append(np.zeros(n))\n",
        "    \n",
        "  def optimize(self, X, y):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    model = self.model\n",
        "    num_layers = len(model.weight)\n",
        "    layer_output = model.forward(X)\n",
        "    dw,db = model.backward(X, y, layer_output)\n",
        "    for l in range(num_layers):\n",
        "      self.v_w[l]=self.beta2*self.v_w[l]+(1-self.beta2)*np.power(dw[l],2)\n",
        "      self.v_b[l]=self.beta2*self.v_b[l]+(1-self.beta2)*np.power(db[l],2)\n",
        "      self.m_w[l]=self.beta1*self.m_w[l]+(1-self.beta1)*dw[l]\n",
        "      self.m_b[l]=self.beta1*self.m_b[l]+(1-self.beta1)*db[l]\n",
        "      self.m_w_hat[l]=(1/(1-(self.beta1**(self.found+1))))*self.m_w[l]\n",
        "      self.m_b_hat[l]=(1/(1-(self.beta1**(self.found+1))))*self.m_b[l]\n",
        "      self.v_w_hat[l]=(1/(1-(self.beta2**(self.found+1))))*self.v_w[l]\n",
        "      self.v_b_hat[l]=(1/(1-(self.beta2**(self.found+1))))*self.v_b[l]\n",
        "      self.mw_cap[l]=self.beta1*self.m_w_hat[l]+(1-self.beta1)*dw[l]\n",
        "      self.mb_cap[l]=self.beta1*self.m_b_hat[l]+(1-self.beta1)*db[l]\n",
        "      model.weight[l]-=(self.alpha/np.sqrt(self.v_w_hat[l]+self.epsilon))*self.mw_cap[l]\n",
        "      model.bias[l]-=(self.alpha/np.sqrt(self.v_b_hat[l]+self.epsilon))*self.mb_cap[l]\n",
        "    self.found=self.found+1\n",
        "  def error(self, X, y):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    batch_size = X.shape[0]\n",
        "    prob = self.model.forward(X)[-1]\n",
        "    err = - np.sum(np.log(prob[np.arange(batch_size), y])) / batch_size\n",
        "    return err"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "##from optimizer import SGD\n",
        "\n",
        "\n",
        "class FNN(object):\n",
        "  def __init__(self, input_size, output_size, hidden_layers_size, reg=0.001):\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.weight, self.bias = None, None\n",
        "    self.initialize(input_size, hidden_layers_size, output_size)\n",
        "    self.reg = reg\n",
        "\n",
        "  def initialize(self, input_size, hidden_layers_size, output_size):\n",
        "    self.weight, self.bias = [], []\n",
        "    prev_layer_size = input_size\n",
        "    hidden_layers_size.append(output_size)\n",
        "    for curr_layer_size in hidden_layers_size:\n",
        "      self.weight.append(np.random.normal(0, 1, size=(prev_layer_size, curr_layer_size)))\n",
        "      self.bias.append(np.zeros(curr_layer_size))\n",
        "      prev_layer_size = curr_layer_size\n",
        "\n",
        "  def reset(self):\n",
        "    num_layers = len(self.weight)\n",
        "    for l in range(num_layers):\n",
        "      m, n = self.weight[l].shape\n",
        "      self.weight[l] = np.random.normal(0, 1, size=(m, n))\n",
        "      self.bias[l] = np.zeros(n)\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid(x):\n",
        "    return 1./(1+np.exp(-x))\n",
        "\n",
        "  @staticmethod\n",
        "  def softmax(x):\n",
        "    \"\"\"\n",
        "    x: (batch_size(B), data_size(N))\n",
        "    \"\"\"\n",
        "    x_max = np.max(x, axis=1, keepdims=True)\n",
        "    exp_prob = np.exp(x - x_max)\n",
        "    prob = exp_prob / np.sum(exp_prob, axis=1, keepdims=True)\n",
        "    return prob\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    \"\"\"\n",
        "    layer_output = []\n",
        "    prev_layer = X\n",
        "    num_hidden_layers = last_layer = len(self.weight) - 1\n",
        "    for t in range(num_hidden_layers):\n",
        "      w, b = self.weight[t], self.bias[t]\n",
        "      next_layer = self.sigmoid(np.dot(prev_layer, w) + b)\n",
        "      layer_output.append(next_layer)\n",
        "      prev_layer = next_layer\n",
        "    w, b = self.weight[last_layer], self.bias[last_layer]\n",
        "    prob = self.softmax(np.dot(prev_layer, w) + b)\n",
        "    layer_output.append(prob)\n",
        "    return layer_output\n",
        "\n",
        "  def backward(self, X, y, layer_output):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    batch_size, _ = X.shape\n",
        "    num_hidden_layers = last_layer = len(layer_output)-1\n",
        "    dw, db = [None]*(num_hidden_layers+1), [None]*(num_hidden_layers+1)\n",
        "    for t in range(num_hidden_layers, -1, -1):\n",
        "      if t == last_layer:\n",
        "        dh = layer_output[t] / batch_size\n",
        "        dh[np.arange(batch_size), y] -= 1/batch_size\n",
        "      else:\n",
        "        dh = np.dot(dh_fwd, self.weight[t+1].T) * layer_output[t] * (1-layer_output[t])\n",
        "      prev_layer_output = X if t==0 else layer_output[t-1]\n",
        "      dw[t] = np.dot(prev_layer_output.T, dh) \n",
        "      # dw[t] = np.dot(prev_layer_output.T, dh) + self.reg*self.weight[t]\n",
        "      db[t] = np.sum(dh, axis=0)\n",
        "      dh_fwd = dh\n",
        "    return dw, db\n",
        "\n",
        "  def error(self, X, y):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    batch_size = X.shape[0]\n",
        "    prob = self.forward(X)[-1]\n",
        "    err = - np.sum(np.log(prob[np.arange(batch_size), y])) / batch_size\n",
        "    # for w in self.weight:\n",
        "    #   err += 0.5 * self.reg * np.sum(np.power(w,2))\n",
        "    return err\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "consider = 10000\n",
        "X = np.array([(x_train[i].flatten())/255for i in range(consider)])\n",
        "Y = y_train[:consider]\n",
        "batch_size, epochs = 16, 20\n",
        "model = FNN(784, 10, [50, 20])\n",
        "##sgd = SGD(model, 0.01)\n",
        "NADAM=NADAM(model,0.0001)\n",
        "\n",
        "\n",
        "for ep in range(1, epochs+1):\n",
        "  ids = np.arange(consider)\n",
        "  np.random.shuffle(ids)\n",
        "  start, end = 0, batch_size\n",
        "  while end > start:\n",
        "    x, y = X[ids[start:end]], Y[ids[start:end]]\n",
        "    NADAM.optimize(x, y)\n",
        "    start, end = end, min(consider, end+batch_size)\n",
        "  # print(model.weight[0])\n",
        "  err = NADAM.error(X, Y)\n",
        "  print(f'epoch: {ep}, error: {err}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrhmnFqtCKX-",
        "outputId": "b2523d55-2d15-4805-e06f-13aedaa0be3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "epoch: 1, error: 2.815581300642716\n",
            "epoch: 2, error: 2.2039960443389863\n",
            "epoch: 3, error: 1.8399932342971297\n",
            "epoch: 4, error: 1.6273174956360354\n",
            "epoch: 5, error: 1.4747348506262705\n",
            "epoch: 6, error: 1.3588426840754724\n",
            "epoch: 7, error: 1.2665572882317733\n",
            "epoch: 8, error: 1.1917473062823982\n",
            "epoch: 9, error: 1.1279871964211157\n",
            "epoch: 10, error: 1.0717075881650653\n",
            "epoch: 11, error: 1.022238320426339\n",
            "epoch: 12, error: 0.9779592715017479\n",
            "epoch: 13, error: 0.9398399162238704\n",
            "epoch: 14, error: 0.9062665476048284\n",
            "epoch: 15, error: 0.876704999813121\n",
            "epoch: 16, error: 0.8501544205283748\n",
            "epoch: 17, error: 0.8264380423062369\n",
            "epoch: 18, error: 0.8047116142391665\n",
            "epoch: 19, error: 0.7843398131719664\n",
            "epoch: 20, error: 0.7658003410820057\n"
          ]
        }
      ]
    }
  ]
}