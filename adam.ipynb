{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adam.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/returaj/cs6910/blob/assginment1_akash/adam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Da3eANsrXVLb"
      },
      "outputs": [],
      "source": [
        "class adam_gd(object):\n",
        "  def __init__(self, model, alpha):\n",
        "    self.model = model\n",
        "    self.alpha = alpha\n",
        "    self.found=0\n",
        "    self.initialize()\n",
        "  \n",
        "  def initialize(self):\n",
        "    self.gamma=0.9\n",
        "    self.epsilon=0.000001\n",
        "    self.beta1=0.9\n",
        "    self.beta2=0.99\n",
        "    self.v_w=[]\n",
        "    self.v_b=[]\n",
        "    self.m_w=[]\n",
        "    self.m_b=[]\n",
        "    self.m_w_hat=[]\n",
        "    self.m_b_hat=[]\n",
        "    self.v_w_hat=[]\n",
        "    self.v_b_hat=[]\n",
        "    num_layers = len(model.weight)\n",
        "    for i in range(num_layers):\n",
        "      m, n = self.model.weight[i].shape\n",
        "      self.v_w.append(np.zeros((m,n)))\n",
        "      self.v_b.append(np.zeros(n))\n",
        "      self.m_w.append(np.zeros((m,n)))\n",
        "      self.m_b.append(np.zeros(n))\n",
        "      self.m_w_hat.append(np.zeros((m,n)))\n",
        "      self.m_b_hat.append(np.zeros(n))\n",
        "      self.v_w_hat.append(np.zeros((m,n)))\n",
        "      self.v_b_hat.append(np.zeros(n))\n",
        "    \n",
        "  def optimize(self, X, y):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    model = self.model\n",
        "    num_layers = len(model.weight)\n",
        "    layer_output = model.forward(X)\n",
        "    dw,db = model.backward(X, y, layer_output)\n",
        "    for l in range(num_layers):\n",
        "      self.v_w[l]=self.beta2*self.v_w[l]+(1-self.beta2)*np.power(dw[l],2)\n",
        "      self.v_b[l]=self.beta2*self.v_b[l]+(1-self.beta2)*np.power(db[l],2)\n",
        "      self.m_w[l]=self.beta1*self.m_w[l]+(1-self.beta1)*dw[l]\n",
        "      self.m_b[l]=self.beta1*self.m_b[l]+(1-self.beta1)*db[l]\n",
        "      self.m_w_hat[l]=(1/(1-(self.beta1**(self.found+1))))*self.m_w[l]\n",
        "      self.m_b_hat[l]=(1/(1-(self.beta1**(self.found+1))))*self.m_b[l]\n",
        "      self.v_w_hat[l]=(1/(1-(self.beta2**(self.found+1))))*self.v_w[l]\n",
        "      self.v_b_hat[l]=(1/(1-(self.beta2**(self.found+1))))*self.v_b[l]\n",
        "      model.weight[l]-=(self.alpha/np.sqrt(self.v_w_hat[l]+self.epsilon))*self.m_w_hat[l]\n",
        "      model.bias[l]-=(self.alpha/np.sqrt(self.v_b_hat[l]+self.epsilon))*self.m_b_hat[l]\n",
        "    self.found=self.found+1\n",
        "  def error(self, X, y):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    batch_size = X.shape[0]\n",
        "    prob = self.model.forward(X)[-1]\n",
        "    err = - np.sum(np.log(prob[np.arange(batch_size), y])) / batch_size\n",
        "    return err"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "##from optimizer import SGD\n",
        "\n",
        "\n",
        "class FNN(object):\n",
        "  def __init__(self, input_size, output_size, hidden_layers_size, reg=0.001):\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.weight, self.bias = None, None\n",
        "    self.initialize(input_size, hidden_layers_size, output_size)\n",
        "    self.reg = reg\n",
        "\n",
        "  def initialize(self, input_size, hidden_layers_size, output_size):\n",
        "    self.weight, self.bias = [], []\n",
        "    prev_layer_size = input_size\n",
        "    hidden_layers_size.append(output_size)\n",
        "    for curr_layer_size in hidden_layers_size:\n",
        "      self.weight.append(np.random.normal(0, 1, size=(prev_layer_size, curr_layer_size)))\n",
        "      self.bias.append(np.zeros(curr_layer_size))\n",
        "      prev_layer_size = curr_layer_size\n",
        "\n",
        "  def reset(self):\n",
        "    num_layers = len(self.weight)\n",
        "    for l in range(num_layers):\n",
        "      m, n = self.weight[l].shape\n",
        "      self.weight[l] = np.random.normal(0, 1, size=(m, n))\n",
        "      self.bias[l] = np.zeros(n)\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid(x):\n",
        "    return 1./(1+np.exp(-x))\n",
        "\n",
        "  @staticmethod\n",
        "  def softmax(x):\n",
        "    \"\"\"\n",
        "    x: (batch_size(B), data_size(N))\n",
        "    \"\"\"\n",
        "    x_max = np.max(x, axis=1, keepdims=True)\n",
        "    exp_prob = np.exp(x - x_max)\n",
        "    prob = exp_prob / np.sum(exp_prob, axis=1, keepdims=True)\n",
        "    return prob\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    \"\"\"\n",
        "    layer_output = []\n",
        "    prev_layer = X\n",
        "    num_hidden_layers = last_layer = len(self.weight) - 1\n",
        "    for t in range(num_hidden_layers):\n",
        "      w, b = self.weight[t], self.bias[t]\n",
        "      next_layer = self.sigmoid(np.dot(prev_layer, w) + b)\n",
        "      layer_output.append(next_layer)\n",
        "      prev_layer = next_layer\n",
        "    w, b = self.weight[last_layer], self.bias[last_layer]\n",
        "    prob = self.softmax(np.dot(prev_layer, w) + b)\n",
        "    layer_output.append(prob)\n",
        "    return layer_output\n",
        "\n",
        "  def backward(self, X, y, layer_output):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    batch_size, _ = X.shape\n",
        "    num_hidden_layers = last_layer = len(layer_output)-1\n",
        "    dw, db = [None]*(num_hidden_layers+1), [None]*(num_hidden_layers+1)\n",
        "    for t in range(num_hidden_layers, -1, -1):\n",
        "      if t == last_layer:\n",
        "        dh = layer_output[t] / batch_size\n",
        "        dh[np.arange(batch_size), y] -= 1/batch_size\n",
        "      else:\n",
        "        dh = np.dot(dh_fwd, self.weight[t+1].T) * layer_output[t] * (1-layer_output[t])\n",
        "      prev_layer_output = X if t==0 else layer_output[t-1]\n",
        "      dw[t] = np.dot(prev_layer_output.T, dh) \n",
        "      # dw[t] = np.dot(prev_layer_output.T, dh) + self.reg*self.weight[t]\n",
        "      db[t] = np.sum(dh, axis=0)\n",
        "      dh_fwd = dh\n",
        "    return dw, db\n",
        "  if __name__ == '__main__':\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    consider = 10000\n",
        "    X = np.array([(x_train[i].flatten())/255 for i in range(consider)])\n",
        "    Y = y_train[:consider]\n",
        "    batch_size, epochs = 16, 20\n",
        "    model = FNN(784, 10, [50, 20])\n",
        "    adam_gd=adam_gd(model,0.01)\n",
        "    for ep in range(1, epochs+1):\n",
        "      ids = np.arange(consider)\n",
        "      np.random.shuffle(ids)\n",
        "      start, end = 0, batch_size\n",
        "      while end > start:\n",
        "        x, y = X[ids[start:end]], Y[ids[start:end]]\n",
        "        adam_gd.optimize(x, y)\n",
        "        start, end = end, min(consider, end+batch_size)\n",
        "      err = adam_gd.error(X, Y)\n",
        "      print(f'epoch: {ep}, error: {err}')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNnNsS-XXr89",
        "outputId": "c1a81f33-66d6-427d-875f-f70a6b1c75b9"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, error: 0.64573300666545\n",
            "epoch: 2, error: 0.5568548881530018\n",
            "epoch: 3, error: 0.5503088799364568\n",
            "epoch: 4, error: 0.5395041153534054\n",
            "epoch: 5, error: 0.49620720104271676\n",
            "epoch: 6, error: 0.4590118344916855\n",
            "epoch: 7, error: 0.4346359010808701\n",
            "epoch: 8, error: 0.4486436559120977\n",
            "epoch: 9, error: 0.4667449973937557\n",
            "epoch: 10, error: 0.43823783211947076\n",
            "epoch: 11, error: 0.44165973509299894\n",
            "epoch: 12, error: 0.4039723802631306\n",
            "epoch: 13, error: 0.4064062385427289\n",
            "epoch: 14, error: 0.3902047044562657\n",
            "epoch: 15, error: 0.40142541688280126\n",
            "epoch: 16, error: 0.367293353993899\n",
            "epoch: 17, error: 0.43430552382437954\n",
            "epoch: 18, error: 0.39516406556969386\n",
            "epoch: 19, error: 0.40295915343985045\n",
            "epoch: 20, error: 0.38357492517351394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = np.array([[1,2,3],[4,5,6]])"
      ],
      "metadata": {
        "id": "4L6keug8ecZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l - np.max(l, axis=1, keepdims=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsqO562w4o_l",
        "outputId": "f3be958c-cef6-499a-f0be-a32f248285a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2, -1,  0],\n",
              "       [-2, -1,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}