{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "momentum.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFhr7hmpY/T6XBy+7/uSpH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/returaj/cs6910/blob/assginment1_akash/momentum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class SGD(object):\n",
        "  def __init__(self, model, alpha):\n",
        "    self.model = model\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def optimize(self, X, y):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    model = self.model\n",
        "    layer_output = model.forward(X)\n",
        "    dw, db = model.backward(X, y, layer_output)\n",
        "    num_layers = len(model.weight)\n",
        "    for l in range(num_layers):\n",
        "      model.weight[l] -= self.alpha * dw[l]\n",
        "      model.bias[l] -= self.alpha * db[l]\n",
        "\n",
        "  def error(self, X, y):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    batch_size = X.shape[0]\n",
        "    prob = self.model.forward(X)[-1]\n",
        "    err = - np.sum(np.log(prob[np.arange(batch_size), y])) / batch_size\n",
        "    return err"
      ],
      "metadata": {
        "id": "V4G4zyYjUNx5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "momentum based minibatch stochastic gradient"
      ],
      "metadata": {
        "id": "pi__Fm69z5cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class mgd(object):\n",
        "  def __init__(self, model, alpha):\n",
        "    self.model = model\n",
        "    self.alpha = alpha\n",
        "  def optimize(self, X, y):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    gamma=0.9\n",
        "    model = self.model\n",
        "    layer_output = model.forward(X)\n",
        "    dw, db = model.backward(X, y, layer_output)\n",
        "    num_layers = len(model.weight)\n",
        "    layers=num_layers\n",
        "    prev_w=[]\n",
        "    prev_b=[]\n",
        "    v_w=[]\n",
        "    v_b=[]\n",
        "    for i in range(num_layers):\n",
        "      m, n = model.weight[i].shape\n",
        "      prev_w.append(np.zeros((m,n)))\n",
        "      prev_b.append(np.zeros(n))\n",
        "    for l in range(num_layers):\n",
        "      v_w.append( gamma*prev_w[l]-self.alpha*dw[l])\n",
        "      v_b.append(gamma*prev_b[l]-self.alpha*db[l])\n",
        "      model.weight[l]-=v_w[l]\n",
        "      model.bias[l]-=v_b[l]\n",
        "      prev_w[l]=v_w[l]\n",
        "      prev_b[l]=v_b[l]\n",
        "  def error(self, X, y):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    batch_size = X.shape[0]\n",
        "    prob = self.model.forward(X)[-1]\n",
        "    err = - np.sum(np.log(prob[np.arange(batch_size), y])) / batch_size\n",
        "    return err\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "paiLJXHkWYUu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "##from optimizer import SGD\n",
        "\n",
        "\n",
        "class FNN(object):\n",
        "  def __init__(self, input_size, output_size, hidden_layers_size):\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.weight, self.bias = None, None\n",
        "    self.initialize(input_size, hidden_layers_size, output_size)\n",
        "\n",
        "  def initialize(self, input_size, hidden_layers_size, output_size):\n",
        "    self.weight, self.bias = [], []\n",
        "    prev_layer_size = input_size\n",
        "    hidden_layers_size.append(output_size)\n",
        "    for curr_layer_size in hidden_layers_size:\n",
        "      self.weight.append(np.random.normal(0, 1, size=(prev_layer_size, curr_layer_size)))\n",
        "      self.bias.append(np.zeros(curr_layer_size))\n",
        "      prev_layer_size = curr_layer_size\n",
        "\n",
        "  def reset(self):\n",
        "    num_layers = len(self.weight)\n",
        "    for l in range(num_layers):\n",
        "      m, n = self.weight[l].shape\n",
        "      self.weight[l] = np.random.normal(0, 1, size=(m, n))\n",
        "      self.bias[l] = np.zeros(n)\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid(x):\n",
        "    return 1./(1+np.exp(-x))\n",
        "\n",
        "  @staticmethod\n",
        "  def softmax(x):\n",
        "    \"\"\"\n",
        "    x: (batch_size(B), data_size(N))\n",
        "    \"\"\"\n",
        "    exp_prob = np.exp(x)\n",
        "    prob = exp_prob / np.sum(exp_prob, axis=1, keepdims=True)\n",
        "    return prob\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    \"\"\"\n",
        "    layer_output = []\n",
        "    prev_layer = X\n",
        "    num_hidden_layers = last_layer = len(self.weight) - 1\n",
        "    for t in range(num_hidden_layers):\n",
        "      w, b = self.weight[t], self.bias[t]\n",
        "      next_layer = self.sigmoid(np.dot(prev_layer, w) + b)\n",
        "      layer_output.append(next_layer)\n",
        "      prev_layer = next_layer\n",
        "    w, b = self.weight[last_layer], self.bias[last_layer]\n",
        "    prob = self.softmax(np.dot(prev_layer, w) + b)\n",
        "    layer_output.append(prob)\n",
        "    return layer_output\n",
        "\n",
        "  def backward(self, X, y, layer_output):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    batch_size, _ = X.shape\n",
        "    num_hidden_layers = last_layer = len(layer_output)-1\n",
        "    dw, db = [None]*(num_hidden_layers+1), [None]*(num_hidden_layers+1)\n",
        "    for t in range(num_hidden_layers, -1, -1):\n",
        "      if t == last_layer:\n",
        "        dh = layer_output[t] / batch_size\n",
        "        dh[np.arange(batch_size), y] -= 1/batch_size\n",
        "      else:\n",
        "        dh = np.dot(dh_fwd, self.weight[t+1].T) * layer_output[t] * (1-layer_output[t])\n",
        "      prev_layer_output = X if t==0 else layer_output[t-1]\n",
        "      dw[t] = np.dot(prev_layer_output.T, dh)\n",
        "      db[t] = np.sum(dh, axis=0)\n",
        "      dh_fwd = dh\n",
        "    return dw, db\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  consider = 1000\n",
        "  X = np.array([x_train[i].flatten() for i in range(consider)])\n",
        "  Y = y_train[:consider]\n",
        "  batch_size, epochs = 16, 20\n",
        "  model = FNN(784, 10, [50, 20])\n",
        "  ##sgd = SGD(model, 0.01)\n",
        "  mgd=mgd(model,0.01)\n",
        "  for ep in range(1, epochs+1):\n",
        "    ids = np.arange(consider)\n",
        "    np.random.shuffle(ids)\n",
        "    start, end = 0, batch_size\n",
        "    while end > start:\n",
        "      x, y = X[ids[start:end]], Y[ids[start:end]]\n",
        "      mgd.optimize(x, y)\n",
        "      start, end = end, min(consider, end+batch_size)\n",
        "    err = mgd.error(X, Y)\n",
        "    print(f'epoch: {ep}, error: {err}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1bZdrC-UkbO",
        "outputId": "68724118-d5e9-4d31-ae0d-10236887a23e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, error: 9.291827724043484\n",
            "epoch: 2, error: 16.244388817730187\n",
            "epoch: 3, error: 23.489979322261245\n",
            "epoch: 4, error: 31.21371086262765\n",
            "epoch: 5, error: 39.52579770629722\n",
            "epoch: 6, error: 48.064722523103924\n",
            "epoch: 7, error: 56.92359330808786\n",
            "epoch: 8, error: 65.87302358594538\n",
            "epoch: 9, error: 74.91599383849324\n",
            "epoch: 10, error: 84.10903218085913\n",
            "epoch: 11, error: 93.23388064997164\n",
            "epoch: 12, error: 102.41762053896437\n",
            "epoch: 13, error: 111.86871093322223\n",
            "epoch: 14, error: 121.43059455214251\n",
            "epoch: 15, error: 131.00599095825947\n",
            "epoch: 16, error: 140.5888995905773\n",
            "epoch: 17, error: 150.15105200624987\n",
            "epoch: 18, error: 159.73252126759806\n",
            "epoch: 19, error: 169.31059702911443\n",
            "epoch: 20, error: 178.8883657922503\n"
          ]
        }
      ]
    }
  ]
}