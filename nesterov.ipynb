{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nesterov.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7qH5kLrMB1gXCnyZjbkN0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/returaj/cs6910/blob/assginment1_akash/nesterov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Dq89F_3kTYBo"
      },
      "outputs": [],
      "source": [
        "class nest_gd(object):\n",
        "  def __init__(self, model, alpha):\n",
        "    self.model = model\n",
        "    self.alpha = alpha\n",
        "  def optimize(self, X, y):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    gamma=0.9\n",
        "    model = self.model\n",
        "    v_w=[]\n",
        "    v_b=[]\n",
        "    prev_w=[]\n",
        "    prev_b=[]\n",
        "    num_layers = len(model.weight)\n",
        "    layers=num_layers\n",
        "    for i in range(num_layers):\n",
        "      m, n = model.weight[i].shape\n",
        "      prev_w.append(np.zeros((m,n)))\n",
        "      prev_b.append(np.zeros(n))\n",
        "    for j in range(num_layers):\n",
        "      v_w.append(gamma*prev_w[j])\n",
        "      v_b.append(gamma*prev_b[j])\n",
        "    w=model.weight\n",
        "    b=model.bias\n",
        "    model.bias=[]\n",
        "    model.weight=[]\n",
        "    for k in range(num_layers):\n",
        "      model.weight.append(w[k]-v_w[k])\n",
        "      model.bias.append(b[k]-v_b[k])\n",
        "    layer_output = model.forward(X)\n",
        "    dw, db = model.backward(X, y, layer_output)\n",
        "    for l in range(num_layers):\n",
        "      v_w.append( gamma*prev_w[l]+self.alpha*dw[l])\n",
        "      v_b.append(gamma*prev_b[l]+self.alpha*db[l])\n",
        "      model.weight[l]-=v_w[l]\n",
        "      model.bias[l]-=v_b[l]\n",
        "      prev_w[l]=v_w[l]\n",
        "      prev_b[l]=v_b[l]\n",
        "  def error(self, X, y):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    batch_size = X.shape[0]\n",
        "    prob = self.model.forward(X)[-1]\n",
        "    err = - np.sum(np.log(prob[np.arange(batch_size), y])) / batch_size\n",
        "    return err"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "##from optimizer import SGD\n",
        "\n",
        "\n",
        "class FNN(object):\n",
        "  def __init__(self, input_size, output_size, hidden_layers_size):\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.weight, self.bias = None, None\n",
        "    self.initialize(input_size, hidden_layers_size, output_size)\n",
        "\n",
        "  def initialize(self, input_size, hidden_layers_size, output_size):\n",
        "    self.weight, self.bias = [], []\n",
        "    prev_layer_size = input_size\n",
        "    hidden_layers_size.append(output_size)\n",
        "    for curr_layer_size in hidden_layers_size:\n",
        "      self.weight.append(np.random.normal(0, 1, size=(prev_layer_size, curr_layer_size)))\n",
        "      self.bias.append(np.zeros(curr_layer_size))\n",
        "      prev_layer_size = curr_layer_size\n",
        "\n",
        "  def reset(self):\n",
        "    num_layers = len(self.weight)\n",
        "    for l in range(num_layers):\n",
        "      m, n = self.weight[l].shape\n",
        "      self.weight[l] = np.random.normal(0, 1, size=(m, n))\n",
        "      self.bias[l] = np.zeros(n)\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid(x):\n",
        "    return 1./(1+np.exp(-x))\n",
        "\n",
        "  @staticmethod\n",
        "  def softmax(x):\n",
        "    \"\"\"\n",
        "    x: (batch_size(B), data_size(N))\n",
        "    \"\"\"\n",
        "    exp_prob = np.exp(x)\n",
        "    prob = exp_prob / np.sum(exp_prob, axis=1, keepdims=True)\n",
        "    return prob\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    \"\"\"\n",
        "    layer_output = []\n",
        "    prev_layer = X\n",
        "    num_hidden_layers = last_layer = len(self.weight) - 1\n",
        "    for t in range(num_hidden_layers):\n",
        "      w, b = self.weight[t], self.bias[t]\n",
        "      next_layer = self.sigmoid(np.dot(prev_layer, w) + b)\n",
        "      layer_output.append(next_layer)\n",
        "      prev_layer = next_layer\n",
        "    w, b = self.weight[last_layer], self.bias[last_layer]\n",
        "    prob = self.softmax(np.dot(prev_layer, w) + b)\n",
        "    layer_output.append(prob)\n",
        "    return layer_output\n",
        "\n",
        "  def backward(self, X, y, layer_output):\n",
        "    \"\"\"\n",
        "    X: (batch_size(B), data_size(N))\n",
        "    y: (batch_size(B))\n",
        "    \"\"\"\n",
        "    batch_size, _ = X.shape\n",
        "    num_hidden_layers = last_layer = len(layer_output)-1\n",
        "    dw, db = [None]*(num_hidden_layers+1), [None]*(num_hidden_layers+1)\n",
        "    for t in range(num_hidden_layers, -1, -1):\n",
        "      if t == last_layer:\n",
        "        dh = layer_output[t] / batch_size\n",
        "        dh[np.arange(batch_size), y] -= 1/batch_size\n",
        "      else:\n",
        "        dh = np.dot(dh_fwd, self.weight[t+1].T) * layer_output[t] * (1-layer_output[t])\n",
        "      prev_layer_output = X if t==0 else layer_output[t-1]\n",
        "      dw[t] = np.dot(prev_layer_output.T, dh)\n",
        "      db[t] = np.sum(dh, axis=0)\n",
        "      dh_fwd = dh\n",
        "    return dw, db\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  consider = 1000\n",
        "  X = np.array([x_train[i].flatten() for i in range(consider)])\n",
        "  Y = y_train[:consider]\n",
        "  batch_size, epochs = 16, 20\n",
        "  model = FNN(784, 10, [50, 20])\n",
        "  ##sgd = SGD(model, 0.01)\n",
        "  nest_gd=nest_gd(model,0.01)\n",
        "\n",
        "\n",
        "  for ep in range(1, epochs+1):\n",
        "    ids = np.arange(consider)\n",
        "    np.random.shuffle(ids)\n",
        "    start, end = 0, batch_size\n",
        "    while end > start:\n",
        "      x, y = X[ids[start:end]], Y[ids[start:end]]\n",
        "      nest_gd.optimize(x, y)\n",
        "      start, end = end, min(consider, end+batch_size)\n",
        "    err = nest_gd.error(X, Y)\n",
        "    print(f'epoch: {ep}, error: {err}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx-L-99JTfJ5",
        "outputId": "70950f0b-6ede-4a60-d199-0dcd024ab5ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, error: 4.6066399367282225\n",
            "epoch: 2, error: 4.6066399367282225\n",
            "epoch: 3, error: 4.6066399367282225\n",
            "epoch: 4, error: 4.6066399367282225\n",
            "epoch: 5, error: 4.6066399367282225\n",
            "epoch: 6, error: 4.6066399367282225\n",
            "epoch: 7, error: 4.6066399367282225\n",
            "epoch: 8, error: 4.6066399367282225\n",
            "epoch: 9, error: 4.6066399367282225\n",
            "epoch: 10, error: 4.6066399367282225\n",
            "epoch: 11, error: 4.6066399367282225\n",
            "epoch: 12, error: 4.6066399367282225\n",
            "epoch: 13, error: 4.6066399367282225\n",
            "epoch: 14, error: 4.6066399367282225\n",
            "epoch: 15, error: 4.6066399367282225\n",
            "epoch: 16, error: 4.6066399367282225\n",
            "epoch: 17, error: 4.6066399367282225\n",
            "epoch: 18, error: 4.6066399367282225\n",
            "epoch: 19, error: 4.6066399367282225\n",
            "epoch: 20, error: 4.6066399367282225\n"
          ]
        }
      ]
    }
  ]
}